


!rm -rf luganda-ner-v6/

!rm -rf wandb/






!pip install datasets
!pip install transformers[sentencepiece]
!apt install git-lfs
!pip install seqeval
!pip install wandb
!pip install accelerate>=0.20.1
!pip install transformers[torch]
!pip install shap
!pip install eli5
!pip install lime





from huggingface_hub import notebook_login

notebook_login()





from datasets import load_metric, load_dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer
import pandas as pd
import numpy as np
import re
import string
import torch
from tqdm.notebook import tqdm
tqdm.pandas()
from sklearn.model_selection import train_test_split
import wandb






wandb.login()
!wandb login --relogin





raw_datasets = load_dataset("Conrad747/lg-ner")


raw_datasets


labels_list = raw_datasets["train"].features[f"ner_tags"].feature.names
labels_list





"""
xlnet performs better than bert
roberta performs better than xlnet
does roberta (cross lingual) perform better than distilbert (monolingual)?
"""
model_checkpoint0 = "Conrad747/luganda-ner-v6"
model_checkpoint1 = "xlm-roberta-large"
model_checkpoint2 = "masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0"
"DeBERTa"

task = "ner"
batch_size = 8





# edit values from here to change the model being worked with
model_checkpoint = model_checkpoint0
huggingface_repo = "luganda-ner-v6"

# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint1)
# for roberta
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)

id2label = {i: label for i, label in enumerate(labels_list)}
label2id = {v: k for k, v in id2label.items()}
print(id2label)
print(label2id)

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True
)

model.config.num_labels

metric = load_metric("seqeval")

def compute_metrics(eval_preds):
    predictions, labels = eval_preds
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [labels_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [labels_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels, scheme='BILOU', mode='strict')
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

'''Here we set the labels of all special tokens to -100
    (the index that is ignored by PyTorch) and the labels of all
    other tokens to the label of the word they come from.
    Another strategy is to set the label only on the first token obtained from a given word,
    and give a label of -100 to the other subtokens from the same word. We propose the two strategies here,
    just change the value of the following flag:
'''
label_all_tokens = True

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f"{task}_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            # We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

    # tokenized_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True)
'''
    Note that we passed batched=True to encode the texts by batches together.
    This is to leverage the full benefit of the fast tokenizer we loaded earlier,
    which will use multi-threading to treat the texts in a batch concurrently.
'''
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)

# add padding to the tokenised inputs
'''
    Batched inputs are often different lengths, so they can’t be converted to fixed-size tensors.
    Padding and truncation are strategies for dealing with this problem,
    to create rectangular tensors from batches of varying lengths.
    Padding adds a special padding token to ensure shorter sequences will have the
    same length as either the longest sequence in a batch or the maximum length accepted by the model.
    Truncation works in the other direction by truncating long sequences.
'''
data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

args = TrainingArguments(
    huggingface_repo,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=10,
    weight_decay=0.01,
    report_to="wandb",
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)

trainer.train()

trainer.evaluate()


from lime.lime_text import LimeTextExplainer
import numpy as np

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch

# Load the Luganda dataset
luganda_dataset = load_dataset("conrad747/lg-ner")

# Extract Luganda texts
luganda_texts = luganda_dataset['train']['tokens']

# Define your model checkpoint
model_checkpoint = "Conrad747/luganda-ner-v6"

# Initialize tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

# Set maximum sequence length
max_seq_length = 128

# Tokenize the dataset and align labels
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, padding="max_length", max_length=max_seq_length, is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Tokenize and align labels for the dataset
tokenized_datasets = luganda_dataset.map(tokenize_and_align_labels, batched=True)

# Initialize LIME explainer
explainer = LimeTextExplainer()

# Function to predict using the model
def predict(texts):
    with torch.no_grad():  # Disable gradient calculation
        inputs = tokenizer(texts, return_tensors="pt", truncation=True, padding=True, max_length=max_seq_length)
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=2)
    return np.array(predictions.tolist())

# Select sample texts for explanation
sample_texts = ["Katikiro Ssebugwaawo asisinkanye Minisita wa Kampala Minsa Kabanda",
                "Bano ejjinja lino balikwasizza Katikkiro Charles Peter Mayiga ku Lwokusatu mu Bulange e Mmengo"]

# Explain the model's predictions using LIME
for text in sample_texts:
    exp = explainer.explain_instance(text, predict, num_features=10, labels=[0, 1, 2, 3, 4, 5])
    exp.show_in_notebook(text=True)



import torch
import numpy as np
import eli5
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Load the Luganda dataset
luganda_dataset = load_dataset("conrad747/lg-ner")

# Define your model checkpoint
model_checkpoint = "Conrad747/luganda-ner-v6"

# Initialize tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

# Set maximum sequence length
max_seq_length = 128

# Tokenize the dataset and align labels
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, padding="max_length", max_length=max_seq_length, is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Tokenize and align labels for the dataset
tokenized_datasets = luganda_dataset.map(tokenize_and_align_labels, batched=True)

# Function to predict using the model
def predict_proba(texts):
    with torch.no_grad():  # Disable gradient calculation
        inputs = tokenizer(texts, return_tensors="pt", truncation=True, padding=True, max_length=max_seq_length)
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    return predictions.tolist()

# Select sample texts for explanation
sample_texts = ["Katikiro Ssebugwaawo asisinkanye Minisita wa Kampala Minsa Kabanda",
                "Bano ejjinja lino balikwasizza Katikkiro Charles Peter Mayiga ku Lwokusatu mu Bulange e Mmengo"]

# Explain the model's predictions using eli5
for text in sample_texts:
    exp = eli5.explain_prediction(predict_proba, text, vec=tokenizer, top_targets=10)
    print(eli5.format_as_text(exp))



import numpy as np
import nltk
nltk.download('punkt')
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification
from nltk.tokenize import word_tokenize
import eli5

# Load the Luganda dataset
luganda_dataset = load_dataset("conrad747/lg-ner")

# Extract Luganda texts
luganda_texts = luganda_dataset['train']['tokens']

# Define your BERT-based model checkpoint
model_checkpoint = "Conrad747/luganda-ner-v6"

# Initialize tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

# Set maximum sequence length
max_seq_length = 128

# Function to predict using the BERT-based model
def predict(texts):
    inputs = tokenizer(texts, return_tensors="pt", truncation=True, padding=True, max_length=max_seq_length)
    with torch.no_grad():  # Disable gradient calculation
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=2)
    return np.array(predictions.tolist())

# Function to explain predictions using eli5
def explain_prediction(text, word_index):
    # Predict probabilities for each class for the given text
    preds = predict([text])[0]
    # Get the word at the target index
    target_word = word_tokenize(text)[word_index]
    # Get the feature names (i.e., tokens)
    feature_names = tokenizer.tokenize(text)
    # Get the weight of each feature (token) for the target word
    weights = np.zeros(len(feature_names))
    for i, feat_name in enumerate(feature_names):
        weights[i] = preds[i] if feat_name == target_word else 0
    # Create a feature weights dictionary
    feature_weights = {feat_name: weight for feat_name, weight in zip(feature_names, weights)}
    # Explain the prediction using eli5
    exp = eli5.explain_weights(feature_weights)
    return exp

# Sample text for explanation
text = "Bano ejjinja lino balikwasizza Katikkiro Charles Peter Mayiga ku Lwokusatu mu Bulange e Mmengo"

# Example usage
for index, word in enumerate(word_tokenize(text)):
    print(f"Explaining for word at index {index}: {word}")
    explanation = explain_prediction(text, index)
    print(explanation)



import eli5
import numpy as np
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Load Luganda NER model and tokenizer
model_checkpoint = "Conrad747/luganda-ner-v6"
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Load Luganda dataset
luganda_dataset = load_dataset("conrad747/lg-ner")

# Define function to preprocess text
def preprocess_text(texts, tokenizer, max_seq_length):
    tokenized_inputs = tokenizer(texts, truncation=True, padding=True, max_length=max_seq_length, is_split_into_words=True)
    return tokenized_inputs

# Define function to predict using the model
def predict(inputs):
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=2)
    return predictions

# Define function to explain predictions using eli5
def explain_predictions(texts, tokenizer, model):
    explanations = []
    for text in texts:
        inputs = preprocess_text([text], tokenizer, max_seq_length=128)
        exp = eli5.explain_prediction(model, inputs)
        explanations.append(exp)
    return explanations

# Example usage
texts = ["Bano ejjinja lino balikwasizza Katikkiro Charles Peter Mayiga ku Lwokusatu mu Bulange e Mmengo"]
explanations = explain_predictions(texts, tokenizer, model)

from IPython.display import display_html

# Define function to display explanations as HTML
def display_explanations(explanations):
    for exp in explanations:
        html_content = eli5.format_as_html(exp)
        display_html(html_content, raw=True)

# Example usage
texts = ["Bano ejjinja lino balikwasizza Katikkiro Charles Peter Mayiga ku Lwokusatu mu Bulange e Mmengo"]
explanations = explain_predictions(texts, tokenizer, model)

# Display explanations
display_explanations(explanations)



import shap
import spacy
from transformers import pipeline

# Load your Luganda NER Hugging Face model
model = "Conrad747/luganda-ner-v6"  # Replace with your model name or path
nlp = pipeline("ner", model=model)

# Define your classes based on your Luganda NER model
classes = ["O", "B-PERSON", "I-PERSON", "L-PERSON", "U-PERSON", "B-NORP", "I-NORP", "L-NORP", "U-NORP", "B-DATE", "I-DATE", "L-DATE", "U-DATE", "B-USERID", "I-USERID", "L-USERID", "U-USERID", "B-ORG", "I-ORG", "L-ORG", "U-ORG", "B-LOCATION", "I-LOCATION", "L-LOCATION", "U-LOCATION"]

# Define a function to predict using the Hugging Face model
def predict(texts):
    results = []
    for doc in nlp(texts):
        results.append({label: 1 if ent["entity"] == label else 0 for label in classes for ent in doc["entities"]})
    return results

# Create a function to create a transformers-like tokenizer to match shap's expectations
def tok_wrapper(text, return_offsets_mapping=False):
    doc = nlp(text)
    out = {"input_ids": [tok["word"] for tok in doc]}

    # Add padding if the input text is shorter than expected
    max_length = 512  # Adjust this based on your model's maximum input length
    padding_token = 0  # Assuming 0 is the ID for padding tokens
    if len(out["input_ids"]) < max_length:
        out["input_ids"] += [padding_token] * (max_length - len(out["input_ids"]))

    if return_offsets_mapping:
        out["offset_mapping"] = [(tok["start"], tok["end"]) for tok in doc]
    return out


# Define the SHAP explainer
explainer = shap.Explainer(
    predict,
    masker=shap.maskers.Text(tok_wrapper),
    algorithm="permutation",
    output_names=classes,  # Use your defined class labels
    max_evals=1500
)

# Example usage
sample_texts = ["Katikiro Ssebugwaawo asisinkanye Minisita wa Kampala Minsa Kabanda", "Bano ejjinja lino balikwasizza Katikkiro Charles Peter Mayiga ku Lwokusatu mu Bulange e Mmengo"]  # Replace with your own sample texts
shap_values = explainer(sample_texts)
shap.plots.text(shap_values)



# To get the precision/recall/f1 computed for each category now that we have
#  finished training, we can apply the same function as before on the result of the predict method:

predictions, labels, _ = trainer.predict(tokenized_datasets["validation"])
predictions = np.argmax(predictions, axis=2)

# Remove ignored index (special tokens)
true_predictions = [
    [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]
true_labels = [
    [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]

results = metric.compute(predictions=true_predictions, references=true_labels, scheme='BILOU', mode='strict')
results


# test
from transformers import pipeline
classify = pipeline('ner', model='e4gl33y3/luganda-ner-v6')



classify("Ssemakula yategese ekivvulu okutalaaga ebitundu omuli Buddu ne Bulemeezi.")





!rm -rf luganda-ner-v6/

!rm -rf wandb/



# edit values from here to change the model being worked with
model_checkpoint = model_checkpoint1
huggingface_repo = "luganda-ner-v6"

# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint1)
# for roberta
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)

id2label = {i: label for i, label in enumerate(labels_list)}
label2id = {v: k for k, v in id2label.items()}
print(id2label)
print(label2id)

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True
)

model.config.num_labels

metric = load_metric("seqeval")

def compute_metrics(eval_preds):
    predictions, labels = eval_preds
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [labels_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [labels_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels, scheme='BILOU', mode='strict')
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

'''Here we set the labels of all special tokens to -100
    (the index that is ignored by PyTorch) and the labels of all
    other tokens to the label of the word they come from.
    Another strategy is to set the label only on the first token obtained from a given word,
    and give a label of -100 to the other subtokens from the same word. We propose the two strategies here,
    just change the value of the following flag:
'''
label_all_tokens = True

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f"{task}_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            # We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

    # tokenized_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True)
'''
    Note that we passed batched=True to encode the texts by batches together.
    This is to leverage the full benefit of the fast tokenizer we loaded earlier,
    which will use multi-threading to treat the texts in a batch concurrently.
'''
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)

# add padding to the tokenised inputs
'''
    Batched inputs are often different lengths, so they can’t be converted to fixed-size tensors.
    Padding and truncation are strategies for dealing with this problem,
    to create rectangular tensors from batches of varying lengths.
    Padding adds a special padding token to ensure shorter sequences will have the
    same length as either the longest sequence in a batch or the maximum length accepted by the model.
    Truncation works in the other direction by truncating long sequences.
'''
data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

args = TrainingArguments(
    huggingface_repo,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=10,
    weight_decay=0.01,
    report_to="wandb",
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)

trainer.train()

trainer.evaluate()


from lime.lime_text import LimeTextExplainer
import numpy as np

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch

# Load the Luganda dataset
luganda_dataset = load_dataset("conrad747/lg-ner")

# Define the model checkpoint
model_checkpoint = "xlm-roberta-large"  # Change this to your desired model checkpoint

# Initialize tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

# Set maximum sequence length
max_seq_length = 128

# Tokenize the dataset and align labels
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, padding="max_length", max_length=max_seq_length, is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Tokenize and align labels for the dataset
tokenized_datasets = luganda_dataset.map(tokenize_and_align_labels, batched=True)

# Initialize LIME explainer
explainer = LimeTextExplainer()

# Function to predict using the model
def predict(texts):
    with torch.no_grad():  # Disable gradient calculation
        inputs = tokenizer(texts, return_tensors="pt", truncation=True, padding=True, max_length=max_seq_length)
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=2)
    return np.array(predictions.tolist())

# Select sample texts for explanation
sample_texts = ["Katikiro Ssebugwaawo asisinkanye Minisita wa Kampala Minsa Kabanda",
                "Bano ejjinja lino balikwasizza Katikkiro Charles Peter Mayiga ku Lwokusatu mu Bulange e Mmengo"]

# Explain the model's predictions using LIME
for text in sample_texts:
    exp = explainer.explain_instance(text, predict, num_features=20, top_labels=3)
    exp.show_in_notebook(text=True)



import numpy as np
import nltk
nltk.download('punkt')
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification
from nltk.tokenize import word_tokenize
import eli5

# Load the Luganda dataset
luganda_dataset = load_dataset("conrad747/lg-ner")

# Extract Luganda texts
luganda_texts = luganda_dataset['train']['tokens']

# Define your BERT-based model checkpoint
model_checkpoint = "xlm-roberta-large"

# Initialize tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

# Set maximum sequence length
max_seq_length = 128

# Function to predict using the BERT-based model
def predict(texts):
    inputs = tokenizer(texts, return_tensors="pt", truncation=True, padding=True, max_length=max_seq_length)
    with torch.no_grad():  # Disable gradient calculation
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=2)
    return np.array(predictions.tolist())

# Function to explain predictions using eli5
def explain_prediction(text, word_index):
    # Predict probabilities for each class for the given text
    preds = predict([text])[0]
    # Get the word at the target index
    target_word = word_tokenize(text)[word_index]
    # Get the feature names (i.e., tokens)
    feature_names = tokenizer.tokenize(text)
    # Get the weight of each feature (token) for the target word
    weights = np.zeros(len(feature_names))
    for i, feat_name in enumerate(feature_names):
        weights[i] = preds[i] if feat_name == target_word else 0
    # Create a feature weights dictionary
    feature_weights = {feat_name: weight for feat_name, weight in zip(feature_names, weights)}
    # Explain the prediction using eli5
    exp = eli5.explain_weights(feature_weights)
    return exp

# Sample text for explanation
text = "Bano ejjinja lino balikwasizza Katikkiro Charles Peter Mayiga ku Lwokusatu mu Bulange e Mmengo"

# Example usage
for index, word in enumerate(word_tokenize(text)):
    print(f"Explaining for word at index {index}: {word}")
    explanation = explain_prediction(text, index)
    print(explanation)



import eli5
import numpy as np
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Load Luganda NER model and tokenizer
model_checkpoint = "xlm-roberta-large"
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Load Luganda dataset
luganda_dataset = load_dataset("conrad747/lg-ner")

# Define function to preprocess text
def preprocess_text(texts, tokenizer, max_seq_length):
    tokenized_inputs = tokenizer(texts, truncation=True, padding=True, max_length=max_seq_length, is_split_into_words=True)
    return tokenized_inputs

# Define function to predict using the model
def predict(inputs):
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=2)
    return predictions

# Define function to explain predictions using eli5
def explain_predictions(texts, tokenizer, model):
    explanations = []
    for text in texts:
        inputs = preprocess_text([text], tokenizer, max_seq_length=128)
        exp = eli5.explain_prediction(model, inputs)
        explanations.append(exp)
    return explanations

# Example usage
texts = ["Bano ejjinja lino balikwasizza Katikkiro Charles Peter Mayiga ku Lwokusatu mu Bulange e Mmengo"]
explanations = explain_predictions(texts, tokenizer, model)

from IPython.display import display_html

# Define function to display explanations as HTML
def display_explanations(explanations):
    for exp in explanations:
        html_content = eli5.format_as_html(exp)
        display_html(html_content, raw=True)

# Example usage
texts = ["Bano ejjinja lino balikwasizza Katikkiro Charles Peter Mayiga ku Lwokusatu mu Bulange e Mmengo"]
explanations = explain_predictions(texts, tokenizer, model)

# Display explanations
display_explanations(explanations)









# edit values from here to change the model being worked with
model_checkpoint = "masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0"
huggingface_repo = "luganda-ner-v6"

# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint1)
# for roberta
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)

id2label = {i: label for i, label in enumerate(labels_list)}
label2id = {v: k for k, v in id2label.items()}
print(id2label)
print(label2id)

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True
)

model.config.num_labels

metric = load_metric("seqeval")

def compute_metrics(eval_preds):
    predictions, labels = eval_preds
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [labels_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [labels_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels, scheme='BILOU', mode='strict')
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

'''Here we set the labels of all special tokens to -100
    (the index that is ignored by PyTorch) and the labels of all
    other tokens to the label of the word they come from.
    Another strategy is to set the label only on the first token obtained from a given word,
    and give a label of -100 to the other subtokens from the same word. We propose the two strategies here,
    just change the value of the following flag:
'''
label_all_tokens = True

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f"{task}_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            # We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

    # tokenized_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True)
'''
    Note that we passed batched=True to encode the texts by batches together.
    This is to leverage the full benefit of the fast tokenizer we loaded earlier,
    which will use multi-threading to treat the texts in a batch concurrently.
'''
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)

# add padding to the tokenised inputs
'''
    Batched inputs are often different lengths, so they can’t be converted to fixed-size tensors.
    Padding and truncation are strategies for dealing with this problem,
    to create rectangular tensors from batches of varying lengths.
    Padding adds a special padding token to ensure shorter sequences will have the
    same length as either the longest sequence in a batch or the maximum length accepted by the model.
    Truncation works in the other direction by truncating long sequences.
'''
data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

args = TrainingArguments(
    huggingface_repo,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=10,
    weight_decay=0.01,
    report_to="wandb",
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)

trainer.train()

trainer.evaluate()


from lime.lime_text import LimeTextExplainer
import numpy as np

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch

# Load the Luganda dataset
luganda_dataset = load_dataset("conrad747/lg-ner")

# Define the model checkpoint
model_checkpoint = model_checkpoint2  # Change this to your desired model checkpoint

# Initialize tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

# Set maximum sequence length
max_seq_length = 128

# Tokenize the dataset and align labels
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, padding="max_length", max_length=max_seq_length, is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Tokenize and align labels for the dataset
tokenized_datasets = luganda_dataset.map(tokenize_and_align_labels, batched=True)

# Initialize LIME explainer
explainer = LimeTextExplainer()

# Function to predict using the model
def predict(texts):
    with torch.no_grad():  # Disable gradient calculation
        inputs = tokenizer(texts, return_tensors="pt", truncation=True, padding=True, max_length=max_seq_length)
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=2)
    return np.array(predictions.tolist())

# Select sample texts for explanation
sample_texts = ["Katikiro Ssebugwaawo asisinkanye Minisita wa Kampala Minsa Kabanda",
                "Bano ejjinja lino balikwasizza Katikkiro Charles Peter Mayiga ku Lwokusatu mu Bulange e Mmengo"]

# Explain the model's predictions using LIME
for text in sample_texts:
    exp = explainer.explain_instance(text, predict, num_features=20, top_labels=3)
    exp.show_in_notebook(text=True)



import eli5
import numpy as np
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification

# Load Luganda NER model and tokenizer
model_checkpoint = model_checkpoint2
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Load Luganda dataset
luganda_dataset = load_dataset("conrad747/lg-ner")

# Define function to preprocess text
def preprocess_text(texts, tokenizer, max_seq_length):
    tokenized_inputs = tokenizer(texts, truncation=True, padding=True, max_length=max_seq_length, is_split_into_words=True)
    return tokenized_inputs

# Define function to predict using the model
def predict(inputs):
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=2)
    return predictions

# Define function to explain predictions using eli5
def explain_predictions(texts, tokenizer, model):
    explanations = []
    for text in texts:
        inputs = preprocess_text([text], tokenizer, max_seq_length=128)
        exp = eli5.explain_prediction(model, inputs)
        explanations.append(exp)
    return explanations

# Example usage
texts = ["Bano ejjinja lino balikwasizza Katikkiro Charles Peter Mayiga ku Lwokusatu mu Bulange e Mmengo"]
explanations = explain_predictions(texts, tokenizer, model)

from IPython.display import display_html

# Define function to display explanations as HTML
def display_explanations(explanations):
    for exp in explanations:
        html_content = eli5.format_as_html(exp)
        display_html(html_content, raw=True)

# Example usage
texts = ["Bano ejjinja lino balikwasizza Katikkiro Charles Peter Mayiga ku Lwokusatu mu Bulange e Mmengo"]
explanations = explain_predictions(texts, tokenizer, model)

# Display explanations
display_explanations(explanations)



!rm -rf luganda-ner-v6/

!rm -rf wandb/

